# forest_app/integrations/llm.py

import json
import logging
from typing import Dict, Any, Optional, Type # Added Type for type hinting

import httpx  # Use httpx for async requests
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    RetryError,
    retry_if_exception,
)  # Import RetryError
from pydantic import (
    BaseModel,
    ValidationError,
    Field,
)  # Import pydantic for response validation

# --- MODIFICATION: Import HTA model ---
# Assuming hta_models.py is created in forest_app/modules/
try:
    from forest_app.modules.hta_models import HTAResponseModel
except ImportError:
    # Fallback if the new module isn't created yet or path is wrong
    logging.getLogger(__name__).warning("HTAResponseModel not found. HTA generation calls might fail validation.")
    # Define a dummy HTAResponseModel if needed for type hints below
    class HTAResponseModel(BaseModel):
        hta_root: dict = {} # Minimal dummy structure
# --- END MODIFICATION ---


# Import settings for API key and endpoint
try:
    from forest_app.config.settings import settings
except ImportError:
    # Fallback if settings module is not found (e.g., during isolated testing)
    logger_init = logging.getLogger(__name__)  # Define logger here
    logger_init.error("Could not import settings. LLM integration will likely fail.")
    # Define dummy settings object
    class DummySettings:
        llm_api_endpoint: str = "http://localhost:11434/api/generate"
        llm_api_key: str = "DUMMY_KEY" # Using the placeholder key
        llm_model_name: str = "mistral"
        llm_connect_timeout: float = 5.0
        llm_read_timeout: float = 45.0
        llm_write_timeout: float = 10.0
        llm_pool_timeout: Optional[float] = None
    settings = DummySettings()

logger = logging.getLogger(__name__)


# --- Custom Exceptions ---
class LLMError(Exception):
    """Base exception for LLM integration errors."""
    pass

class LLMResponseFormatError(LLMError):
    """Raised when LLM response is not valid JSON."""
    pass

class LLMValidationError(LLMError):
    """Raised when LLM response JSON does not match the expected schema."""
    def __init__(self, message, errors=None, data=None):
        super().__init__(message)
        self.errors = errors
        self.data = data

class LLMClientError(LLMError):
    """Raised for 4xx errors from the LLM API."""
    pass

class LLMServerError(LLMError):
    """Raised for 5xx errors from the LLM API (after retries)."""
    pass


# --- Pydantic Models for Expected Responses ---

class LLMResponseModel(BaseModel):
    """
    Pydantic model to validate the specific structure expected from the Arbiter LLM.
    Ensures 'task' (as a dict) and 'narrative' (as a string) are present.
    """
    task: Dict[str, Any] = Field(..., description="The task object generated by the Arbiter")
    narrative: str = Field(..., description="The poetic narrative generated by the Arbiter")

    class Config:
        extra = "ignore"

# --- ADDED: Model specifically for Sentiment Analysis results ---
class SentimentResponseModel(BaseModel):
    """Pydantic model for the expected sentiment analysis structure."""
    emotional_fingerprint: Dict[str, float] = Field(..., description="Mapping of core emotional tags to scores")
    shadow_data: Dict[str, Any] = Field(..., description="Contains active_shadow_tags list and shadow_intensity float")
    sentiment_flow: str = Field(..., description="Descriptor of sentiment flow (e.g., improving, stable)")
    ambivalence_score: float = Field(..., description="Conflict score (0.0-1.0)")
    final_score: float = Field(..., description="Overall sentiment score (-1.0 to 1.0)")

    class Config:
        extra = "ignore"
# --- End Added Model ---

# Note: HTAResponseModel is imported from hta_models.py above


# --- Retry Configuration ---
RETRYABLE_STATUS_CODES = {500, 502, 503, 504, 408, 429}

def is_retryable_exception(exception: BaseException) -> bool:
    """Check if an httpx exception is retryable"""
    if isinstance(exception, httpx.TimeoutException):
        logger.warning("LLM request timed out, retrying...")
        return True
    if isinstance(exception, httpx.NetworkError):
        logger.warning("LLM request network error, retrying... Error: %s", exception)
        return True
    if isinstance(exception, httpx.HTTPStatusError):
        is_retryable = exception.response.status_code in RETRYABLE_STATUS_CODES
        if is_retryable:
            logger.warning("LLM request failed with status %d, retrying...", exception.response.status_code)
        return is_retryable
    # Do not retry validation errors
    if isinstance(exception, (LLMValidationError, LLMResponseFormatError)):
         return False
    if isinstance(exception, httpx.RequestError):
        logger.warning("LLM request encountered RequestError, retrying... Error: %s", exception)
        return True
    return False


# --- Core LLM Interaction Function ---
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception(is_retryable_exception),
)
# --- MODIFIED function signature and return type hint ---
async def generate_response(
    prompt: str,
    response_model: Type[BaseModel] = LLMResponseModel # Accept model type, default to original
) -> BaseModel: # Return generic BaseModel
# --- END MODIFICATION ---
    """
    Makes an asynchronous POST request to the configured LLM API endpoint.

    Handles retries, timeouts, specific errors, and validates the response
    against the provided Pydantic response_model.

    Args:
        prompt: The input prompt string for the LLM.
        response_model: The Pydantic model class to validate the response against.
                        Defaults to LLMResponseModel (task/narrative).

    Returns:
        A validated Pydantic object matching the response_model type.

    Raises:
        ValueError: If LLM configuration is missing (endpoint is empty/None).
        LLMResponseFormatError: If the response is not valid JSON.
        LLMValidationError: If the JSON does not match the expected schema (response_model).
        LLMClientError: For non-retryable 4xx errors.
        LLMServerError: For 5xx errors after retries.
        httpx.TimeoutException: If the request times out after retries.
        RetryError: If retries are exhausted for retryable network/server errors.
        LLMError: For other LLM-related errors.
        Exception: For other unexpected errors.
    """
    if not settings.llm_api_endpoint:
        logger.error("LLM API endpoint is not configured in settings.")
        raise ValueError("LLM configuration missing: API endpoint.")

    headers = {"Content-Type": "application/json"}
    if settings.llm_api_key and settings.llm_api_key not in ["", "DUMMY_KEY", "DUMMY_KEY_NO_AUTH"]:
        headers["Authorization"] = f"Bearer {settings.llm_api_key}"

    payload = {
        "model": getattr(settings, "llm_model_name", "mistral"),
        "prompt": prompt,
        "stream": False,
        "format": "json",
        "options": {}
    }

    timeout_config = httpx.Timeout(
        connect=getattr(settings, "llm_connect_timeout", 5.0),
        read=getattr(settings, "llm_read_timeout", 45.0), # Increase if needed
        write=getattr(settings, "llm_write_timeout", 10.0),
        pool=getattr(settings, "llm_pool_timeout", None) # Keep pool param
    )

    logger.info("Sending request to LLM endpoint: %s for model %s (expecting %s)",
                settings.llm_api_endpoint, payload["model"], response_model.__name__)
    logger.debug("LLM Request Payload: %s", payload)

    response_json = {}

    try:
        async with httpx.AsyncClient(timeout=timeout_config) as client:
            response = await client.post(
                settings.llm_api_endpoint, json=payload, headers=headers
            )

        logger.debug("LLM raw response status code: %s", response.status_code)

        if 400 <= response.status_code < 500 and response.status_code not in RETRYABLE_STATUS_CODES:
            logger.error("LLM API Client Error (%s): %s", response.status_code, response.text[:1000])
            raise LLMClientError(f"LLM client error {response.status_code}: {response.text[:200]}")

        response.raise_for_status()

        response_text = response.text
        if not response_text:
             logger.error("LLM returned an empty response body.")
             raise LLMResponseFormatError("LLM returned empty response.")

        logger.debug("LLM raw response text received (first 500 chars): %s", response_text[:500])

        try:
            # Handle Ollama's nested JSON response when format=json is used
            outer_json = json.loads(response_text)
            if 'response' in outer_json and isinstance(outer_json['response'], str):
                 inner_json_str = outer_json['response']
                 try:
                     inner_json_str_cleaned = inner_json_str.strip().removeprefix('```json').removesuffix('```').strip()
                     response_json = json.loads(inner_json_str_cleaned)
                 except json.JSONDecodeError as inner_jde:
                      logger.error("Failed to parse nested JSON within 'response' field: %s. Inner string cleaned: %s", inner_jde, inner_json_str_cleaned[:500])
                      raise LLMResponseFormatError(f"LLM nested JSON decode error: {inner_jde}") from inner_jde
            else:
                 logger.warning("LLM response did not contain expected 'response' field with nested JSON. Attempting to parse outer structure directly.")
                 response_json = outer_json

            # --- MODIFIED Validation Step ---
            # Validate structure using the *passed* response_model type
            validated_data = response_model(**response_json)
            logger.info("LLM response successfully parsed and validated against %s.", response_model.__name__)
            return validated_data
            # --- END MODIFICATION ---

        except json.JSONDecodeError as jde:
            logger.error("LLM response was not valid JSON: %s. Raw text: %s", jde, response_text[:500])
            raise LLMResponseFormatError(f"LLM response decode error: {jde}") from jde
        except ValidationError as ve:
            logger.error(
                "LLM response structure validation failed against %s: %s",
                response_model.__name__, # Log which model failed
                ve
            )
            logger.error("Received data for validation: %s", response_json)
            # Re-raise with specific context
            raise LLMValidationError(
                f"LLM response validation error against {response_model.__name__}",
                errors=ve.errors(),
                data=response_json,
            ) from ve

    except RetryError as retry_err:
        logger.error("LLM request failed after multiple retries. Final error: %s", retry_err.last_attempt.exception())
        raise LLMError("LLM request failed after retries") from retry_err
    except httpx.HTTPStatusError as status_err:
        if status_err.response.status_code >= 500:
            logger.error("LLM API Server Error (%s) occurred after retries: %s", status_err.response.status_code, status_err.response.text[:500])
            raise LLMServerError(f"LLM server error {status_err.response.status_code} after retries") from status_err
        else: # Should be non-retryable 4xx caught by LLMClientError raise earlier
            logger.error("Unhandled HTTPStatusError: %s", status_err)
            raise LLMError(f"Unhandled HTTP Error {status_err.response.status_code}") from status_err
    except httpx.TimeoutException as te:
        logger.error("LLM API request timed out after retries: %s", te)
        raise te
    except httpx.RequestError as req_err:
        logger.error("LLM API request failed due to RequestError after retries: %s", req_err)
        raise LLMError("LLM network/request error after retries") from req_err
    except LLMValidationError as llm_ve:
         logger.error("Caught specific LLMValidationError during processing: %s", llm_ve)
         raise
    except LLMError as llm_e:
         logger.error("Caught specific LLMError during processing: %s", llm_e)
         raise
    except Exception as e:
        logger.exception("An unexpected error occurred during LLM request processing: %s", e)
        raise LLMError("Unexpected error during LLM interaction") from e